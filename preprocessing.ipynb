{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nMI7CYxcyiUB"
      },
      "source": [
        "Before running the file Upload all your data set on your goole drive in a zip format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YjtnZQkTu6tX"
      },
      "outputs": [],
      "source": [
        "#Mount our google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "NVIDIA GeForce GTX 1050\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f4y_fGlmur4v"
      },
      "outputs": [],
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "url = '1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07'\n",
        "gdd.download_file_from_google_drive(file_id = url,dest_path='./data.zip',unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1f40EeRuvAkO"
      },
      "outputs": [],
      "source": [
        "#To get the average frame count \n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "#change the path accordingly\n",
        "video_files =  glob.glob('/content/drive/MyDrive/DeepFake/Dataset/*.mp4')\n",
        "#video_files1 =  glob.glob('/content/dfdc_train_part_0/*.mp4')\n",
        "#video_files += video_files1\n",
        "frame_count = []\n",
        "for video_file in video_files:\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<150):\n",
        "    video_files.remove(video_file)\n",
        "    continue\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "print(\"frames\" , frame_count)\n",
        "print(\"Total number of videos: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in d:\\python\\lib\\site-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in d:\\python\\lib\\site-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Iterate over each video file\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m video_files:\n\u001b[1;32m---> 18\u001b[0m     cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Check if the video has at least 150 frames\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m150\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# RUNNING LOCALLY\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define the path to the video files\n",
        "video_folder = \"D:\\\\DeepFake\\\\Dataset\"\n",
        "\n",
        "# Get a list of all .mp4 files in the folder\n",
        "video_files = [os.path.join(video_folder, file) for file in os.listdir(video_folder) if file.endswith('.mp4')]\n",
        "\n",
        "# Initialize an empty list to store frame counts\n",
        "frame_count = []\n",
        "\n",
        "# Iterate over each video file\n",
        "for video_file in video_files:\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    # Check if the video has at least 150 frames\n",
        "    if int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) < 150:\n",
        "        continue\n",
        "    frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "\n",
        "# Print the frame counts and average frame count\n",
        "print(\"Frames per video:\", frame_count)\n",
        "print(\"Total number of videos:\", len(frame_count))\n",
        "print(\"Average frames per video:\", np.mean(frame_count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With Cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames per video: [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 299, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 301, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 299, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 301, 299, 300, 300, 300, 299, 300, 300, 300, 301, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 299, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 299, 299, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 299, 299, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 299, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 299, 299, 300, 301, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 301, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 299, 300, 300, 300, 300, 300, 300, 300, 299]\n",
            "Total number of videos: 2346\n",
            "Average frames per video: 299.9842284739983\n"
          ]
        }
      ],
      "source": [
        "#with cuda\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Define the path to the video files\n",
        "video_folder = \"D:\\\\DeepFake\\\\Dataset\"\n",
        "\n",
        "# Get a list of all .mp4 files in the folder\n",
        "video_files = [os.path.join(video_folder, file) for file in os.listdir(video_folder) if file.endswith('.mp4')]\n",
        "\n",
        "# Initialize an empty list to store frame counts\n",
        "frame_count = []\n",
        "\n",
        "# Move operations to CUDA GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Iterate over each video file\n",
        "for video_file in video_files:\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    # Check if the video has at least 150 frames\n",
        "    if int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) < 150:\n",
        "        continue\n",
        "    frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "\n",
        "# Print the frame counts and average frame count\n",
        "print(\"Frames per video:\", frame_count)\n",
        "print(\"Total number of videos:\", len(frame_count))\n",
        "print(\"Average frames per video:\", np.mean(frame_count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import face_recognition\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the function to extract frames\n",
        "def frame_extract(path):\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    success = 1\n",
        "    while success:\n",
        "        success, image = vidObj.read()\n",
        "        if success:\n",
        "            yield image\n",
        "\n",
        "# Check if face_recognition is installed, if not, install it\n",
        "try:\n",
        "    import face_recognition\n",
        "except ImportError:\n",
        "    !pip install face_recognition\n",
        "    import face_recognition\n",
        "\n",
        "# Create directory to store processed videos\n",
        "output_dir = 'D:/DeepFake/OnlyFace'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Process the frames\n",
        "def create_face_videos(path_list, out_dir):\n",
        "    already_present_count = len(os.listdir(out_dir))\n",
        "    print(\"Number of videos already present:\", already_present_count)\n",
        "    for path in tqdm(path_list):\n",
        "        out_path = os.path.join(out_dir, os.path.basename(path))\n",
        "        if os.path.exists(out_path):\n",
        "            print(\"File already exists:\", out_path)\n",
        "            continue\n",
        "        frames = []\n",
        "        flag = 0\n",
        "        face_all = []\n",
        "        frames1 = []\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (112, 112))\n",
        "        for idx, frame in enumerate(frame_extract(path)):\n",
        "            if idx <= 150:\n",
        "                frames.append(frame)\n",
        "                if len(frames) == 4:\n",
        "                    faces = face_recognition.batch_face_locations(frames)\n",
        "                    for i, face in enumerate(faces):\n",
        "                        if len(face) != 0:\n",
        "                            top, right, bottom, left = face[0]\n",
        "                            try:\n",
        "                                out.write(cv2.resize(frames[i][top:bottom, left:right, :], (112, 112)))\n",
        "                            except:\n",
        "                                pass\n",
        "                    frames = []\n",
        "        out.release()\n",
        "\n",
        "# Call the function to process videos\n",
        "create_face_videos(video_files, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U92Ovn3JvV52"
      },
      "outputs": [],
      "source": [
        "# to extract frame\n",
        "def frame_extract(path):\n",
        "  vidObj = cv2.VideoCapture(path) \n",
        "  success = 1\n",
        "  while success:\n",
        "      success, image = vidObj.read()\n",
        "      if success:\n",
        "          yield image\n",
        "!pip3 install face_recognition\n",
        "!mkdir '/content/drive/My Drive/FF_REAL_Face_only_data'\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "from tqdm.autonotebook import tqdm\n",
        "# process the frames\n",
        "def create_face_videos(path_list,out_dir):\n",
        "  already_present_count =  glob.glob(out_dir+'*.mp4')\n",
        "  print(\"No of videos already present \" , len(already_present_count))\n",
        "  for path in tqdm(path_list):\n",
        "    out_path = os.path.join(out_dir,path.split('/')[-1])\n",
        "    file_exists = glob.glob(out_path)\n",
        "    if(len(file_exists) != 0):\n",
        "      print(\"File Already exists: \" , out_path)\n",
        "      continue\n",
        "    frames = []\n",
        "    flag = 0\n",
        "    face_all = []\n",
        "    frames1 = []\n",
        "    out = cv2.VideoWriter(out_path,cv2.VideoWriter_fourcc('M','J','P','G'), 30, (112,112))\n",
        "    for idx,frame in enumerate(frame_extract(path)):\n",
        "      #if(idx % 3 == 0):\n",
        "      if(idx <= 150):\n",
        "        frames.append(frame)\n",
        "        if(len(frames) == 4):\n",
        "          faces = face_recognition.batch_face_locations(frames)\n",
        "          for i,face in enumerate(faces):\n",
        "            if(len(face) != 0):\n",
        "              top,right,bottom,left = face[0]\n",
        "            try:\n",
        "              out.write(cv2.resize(frames[i][top:bottom,left:right,:],(112,112)))\n",
        "            except:\n",
        "              pass\n",
        "          frames = []\n",
        "    try:\n",
        "      del top,right,bottom,left\n",
        "    except:\n",
        "      pass\n",
        "    out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sF5qiWGLvei-"
      },
      "outputs": [],
      "source": [
        "create_face_videos(video_files,'/content/drive/My Drive/FF_REAL_Face_only_data/')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
